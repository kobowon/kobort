{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "abc9d41e-88ee-443f-870f-e1ea8cdf39fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('original/vocab.txt', 'r') as f:\n",
    "    subwords = f.readlines()\n",
    "    subwords = [subword.strip() for subword in subwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "9daff93b-2bda-4068-a1a6-b409377e057f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\n",
    "        pretrained_model_name_or_path='original/vocab.txt',\n",
    "        do_lower_case=False,\n",
    "        unk_token=\"<unk>\",\n",
    "        sep_token=\"</s>\",\n",
    "        pad_token=\"<pad>\",\n",
    "        cls_token=\"<s>\",\n",
    "        mask_token=\"<mask>\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "3b292d35-3dbe-43cf-b04a-a5053f0fa1f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ummm\n",
      "['민주주의', '##의', '<unk>']\n"
     ]
    }
   ],
   "source": [
    "result = tokenizer.tokenize(\"민주주의의 첢첢\")[0]\n",
    "if '<unk>' in result:\n",
    "    print(\"ummm\")\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "406490b7-0b8a-484e-bd89-029cdd867671",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['이야말로', '에게서', '한테서', '에다가', '이라고', '으로서', '으로써', '으로서', '이든지', '이나마', '이라도', '야말로', '은커녕', '는커녕', '에서', '으로', '에게', '한테', '께서', '하고', '이랑', '부터', '까지', '이나', '마다', '같이', '처럼', '보다', '밖에', '대로', '이서', '만큼', '더러', '라고', '로서', '로써', '로서', '이자', '이며', '말고', '마저', '조차', '따라', '치고', '이란', '든지', '나마', '라도', '인들', '이', '가', '을', '를', '은', '는', '에', '로', '의', '께', '과', '와', '랑', '도', '만', '서', '자', '며', '뿐', '란']\n"
     ]
    }
   ],
   "source": [
    "josa_list = ['이', '가', '을', '를', '은', '는', '에', '에서', '으로', '로', '의' ,'에게' ,'한테' ,'께', '께서',\n",
    "             '에게서', '한테서', '과', '와', '하고', '이랑', '랑', '도' ,'만', '부터', '까지' ,'이나' ,'마다', '같이',\n",
    "             '처럼', '보다', '밖에', '대로', '이서' ,'서', '에다가' ,'만큼', '더러', '이라고', '라고', '으로서', '로서',\n",
    "             '으로써', '로써', '으로서', '로서', '이자', '자', '이며', '며', '말고', '마저', '조차', '뿐', '따라', '치고', '이란', '란',\n",
    "             '이든지', '든지', '이나마', '나마', '이라도', '라도', '이야말로', '야말로', '은커녕', '는커녕', '인들']\n",
    "\n",
    "josa_list = sorted(josa_list, key=lambda x : len(x), reverse=True)\n",
    "print(josa_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e93a676-0500-44b5-9ed4-aaeda6fdb0a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "e04f0858-a128-4ee7-ad77-0c4ff84279f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_vocab(i, check_subword, subwords):\n",
    "    if check_subword in subwords[:i]:\n",
    "        return True\n",
    "    else:\n",
    "        result = tokenizer.tokenize(check_subword)[0]\n",
    "        if '<unk>' in result:\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "\n",
    "def is_vocab2(i, check_subword, subwords):\n",
    "    if check_subword in subwords[:i]:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "b62e0aa1-ce7a-469a-b8a8-3d71f42bbd71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_vocab(2,'에',['아','에','아에'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "a6d83a20-ac74-40f7-bac1-88a36074cb63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'나는'"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'나는요'[:len('나는요')-len('요')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "33a8eee3-273a-4da4-ab81-13754a5d7b42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48270\n",
      "1730\n"
     ]
    }
   ],
   "source": [
    "max_flag = len(josa_list)\n",
    "\n",
    "new_subwords = []\n",
    "cnt = 0\n",
    "for i, subword in enumerate(subwords):\n",
    "    flag = 0\n",
    "    if '##' not in subword: # ##이 subword에 없는 경우\n",
    "        if len(subword) > 2: #세 글자 이상의 단어에 대해서만 알고리즘 적용함 (부자에서 자도 쪼개버리는 경우가 있어서)\n",
    "            for josa in josa_list:\n",
    "                if subword.endswith(josa):\n",
    "                    if len(subword) > len(josa): #조사를 찾지 않기 위함\n",
    "                        check_subword = subword[:len(subword)-len(josa)] #조사 제외한 스트링\n",
    "                        #아래 if문\n",
    "                        #조사가 vocab에 없어 (조사는 i 위에 있을 필요없음 예를 들어 그나마는 (그나)(마)로 구성됨 (그)(나마)가 아니므로 나마가 위에 안 올 수 있음 \n",
    "                        #하지만 이건 내가 유도한 바가 아님 그래서 해당 조사가 있으면 vocab 에서 찾아보고 없으면 unused token에서 바꿔주면 됨\n",
    "                        if is_vocab(i, check_subword, subwords) and is_vocab(i, '##'+josa, subwords):\n",
    "                            #print(subword)\n",
    "                            cnt += 1\n",
    "                        else: ## 조사를 뗀 토큰 혹은 ##조사가 vocab에 없으면 그냥 분리하지말고 저장 이건 조사가 합쳐져서 생긴 것이 아니기 때문\n",
    "                            new_subwords.append(subword)\n",
    "                    elif len(subword) == len(josa): ##이건 조사로 쓰인게 아님 조사는 ##으로 시작하지만 우리가 저장한 리스트는 ##이 포함되어 있지 않음\n",
    "                        new_subwords.append(subword)\n",
    "                    break\n",
    "                else: #조사로 끝나지 않으면 flag +1\n",
    "                    flag += 1\n",
    "            if flag == max_flag: #조사로 끝나지 않는 단어는 저장\n",
    "                new_subwords.append(subword)\n",
    "        else: ## len(subword) <= 2\n",
    "            new_subwords.append(subword)\n",
    "            \n",
    "    elif '##' in subword: # ##이 subword에 있는 경우도 저장\n",
    "        for jasa in josa_list:\n",
    "            if subword.endswith(josa):\n",
    "                if len(subword)-2 > len(josa):\n",
    "                    check_subword = subword[:len(subword)-len(josa)]\n",
    "                    if is_vocab2(i, check_subword, subwords) and is_vocab2(i, '##'+josa, subwords):\n",
    "                        #print(subword)\n",
    "                        cnt += 1\n",
    "                    else:\n",
    "                        new_subwords.append(subword)\n",
    "                elif subword == '##'+josa:\n",
    "                    new_subwords.append(subword)\n",
    "                break\n",
    "            else: #조사로 끝나지 않으면 flag +1\n",
    "                flag += 1\n",
    "        if flag == max_flag: #조사로 끝나지 않는 단어는 저장\n",
    "            new_subwords.append(subword)\n",
    "\n",
    "print(len(new_subwords))\n",
    "print(cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "842462f5-89f5-46c1-8c5a-327e93ecd11c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'으'.endswith('으')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "0b05b092-db18-4744-84e9-c2afa7d03b46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1730\n",
      "50000 50000 48270 48270\n"
     ]
    }
   ],
   "source": [
    "sub_set = set(subwords)\n",
    "new_set = set(new_subwords)\n",
    "print(len(sub_set-new_set))\n",
    "\n",
    "print(len(subwords), len(sub_set), len(new_subwords), len(new_set))\n",
    "\n",
    "hash = {}\n",
    "for i, sw in enumerate(new_subwords):\n",
    "    if sw not in hash:\n",
    "        hash[sw] = 1\n",
    "    else:\n",
    "        hash[sw] += 1\n",
    "\n",
    "for k,v in hash.items():\n",
    "    if v > 1:\n",
    "        print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "964240cc-899e-4ca3-b104-46a406ebb63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cha = list(sub_set - new_set)\n",
    "#print(cha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "fc44a680-9e51-468d-8b69-c1df7c9eb25c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['제국주의', '원자로', '레이나', '집단주의', '걸으며', '불러서', '깽깽이', '어서라도', '시켰으며', '떠돌이', '닌텐도', '객관적으로', '초현실주의', '유발자', '딸랑딸랑', '유니클로', '할려는', '제주도', '출연자', '유단자', '시위자', '곡괭이', '전라도', '불러다가', '야요이', '자유민주주의', '훔쳐서', '생산자', '막아서', '들썩이', '계승자', '한데도', '돌연변이', '저래도', '떠벌이', '해상도', '따라가', '살인자', '자살자', '라면서', '이력서', '주권자', '더니만', '여기자', '아카이', '위키백과', '지배자', '역사가', '눈덩이', '발주자']\n"
     ]
    }
   ],
   "source": [
    "print(cha[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "01603a3a-cf72-42db-82f0-51729ed975b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['복덕방', '레반', '뭇매', '참살', '서경덕', '볼지', '영력', '피날레', '##사쿠', '인터폴']\n"
     ]
    }
   ],
   "source": [
    "print(new_subwords[-30:-20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "712e7c63-9ef7-481f-bf66-a85f2bad411f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_file = 'v4/vocab.txt'\n",
    "with open(vocab_file,'w',encoding='utf-8') as f:\n",
    "    for w in new_subwords:  \n",
    "        f.write(w+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f0ac608b-af08-427b-9f8f-b432091c84b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "module = importlib.import_module(f\".bert\", \"transformers.models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "abff67c9-2edb-4955-9e93-e7d593d38942",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getattribute_from_module(module, attr):\n",
    "    if attr is None:\n",
    "        print('1')\n",
    "        return None\n",
    "    if isinstance(attr, tuple):\n",
    "        print('2')\n",
    "        return tuple(getattribute_from_module(module, a) for a in attr)\n",
    "    if hasattr(module, attr):\n",
    "        print('3')\n",
    "        return getattr(module, attr)\n",
    "    # Some of the mappings have entries model_type -> object of another model type. In that case we try to grab the\n",
    "    # object at the top level.\n",
    "    transformers_module = importlib.import_module(\"transformers\")\n",
    "    return getattribute_from_module(transformers_module, attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c9863b6e-1cb1-4cdb-943a-8fd72b6b414f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "<class 'transformers.models.bert.modeling_bert.BertForMaskedLM'>\n"
     ]
    }
   ],
   "source": [
    "print(getattribute_from_module(module, 'BertForMaskedLM'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf8c00f-92ac-4525-aa95-523d30917abd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
