{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "bddaa297-5211-4215-86f3-ad943d120b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_ner_tagged_sentence(origin_text, tokens, labels):\n",
    "    ner_tagged_sentence = ''\n",
    "    ner_tagged_id_list = []\n",
    "    \n",
    "    #divide label per character token\n",
    "    #space를 제외한 모든 글자에 대한 라벨을 붙이기\n",
    "    #abc : B-LOC → a : B-LOC, b : I-LOC, c : I-LOC\n",
    "    char_labels = []\n",
    "    char_tokens = []\n",
    "    for token, label in zip(tokens, labels):\n",
    "        if token == '<unk>':\n",
    "            # char_tokens.append('<unk>')\n",
    "            # char_labels.append('O')\n",
    "            continue\n",
    "            \n",
    "        if label == 'O':\n",
    "            for i in range(len(token)):\n",
    "                #한글자씩 분해시킨 토큰\n",
    "                char_token = token[i]\n",
    "                char_label = 'O'\n",
    "                if char_token != \"#\":\n",
    "                    char_labels.append(char_label)\n",
    "                    char_tokens.append(char_token)\n",
    "        elif label.startswith(\"B-\"):\n",
    "            entity = label[2:]\n",
    "            for i in range(len(token)):\n",
    "                char_token = token[i]\n",
    "                if char_token != \"#\":\n",
    "                    if i == 0:\n",
    "                        char_label = \"B-\"+ entity\n",
    "                    elif i > 0:\n",
    "                        char_label = \"I-\"+ entity\n",
    "                elif char_token == \"#\":\n",
    "                    if i < 2:\n",
    "                        continue\n",
    "                    elif i == 2:\n",
    "                        char_label = \"B-\"+ entity\n",
    "                    elif i > 2:\n",
    "                        char_label = \"I-\"+ entity\n",
    "                        \n",
    "                char_labels.append(char_label)\n",
    "                char_tokens.append(char_token)\n",
    "        elif label.startswith(\"I-\"):\n",
    "            entity = label[2:]\n",
    "            for i in range(len(token)):\n",
    "                char_token = token[i]\n",
    "                char_label = \"I-\"+entity\n",
    "                if char_token != \"#\":\n",
    "                    char_tokens.append(char_token)\n",
    "                    char_labels.append(char_label)\n",
    "\n",
    "    #process for <unk>\n",
    "    char_tokens_processed = []\n",
    "    char_labels_processed = []\n",
    "    c_i = 0\n",
    "    for i in range(len(origin_text)):\n",
    "        #space가 아닌 글자에 대해서\n",
    "        origin_char = origin_text[i]\n",
    "        if origin_char != ' ':\n",
    "            #is_unk = True\n",
    "            # try:\n",
    "            if c_i == len(char_tokens):\n",
    "                char_tokens_processed.append('<unk>')\n",
    "                char_labels_processed.append('O')\n",
    "                # print(char_tokens_processed)\n",
    "                # print(char_labels_processed)\n",
    "                continue\n",
    "\n",
    "            if origin_char == char_tokens[c_i]:\n",
    "                #is_unk = False #해당 origin_char는 unk이 아님\n",
    "                char_tokens_processed.append(char_tokens[c_i])\n",
    "                char_labels_processed.append(char_labels[c_i])\n",
    "                #if c_i < len(char_tokens)-1:\n",
    "                c_i += 1\n",
    "                # print(char_tokens_processed)\n",
    "                # print(char_labels_processed)\n",
    "                \n",
    "\n",
    "            elif origin_char != char_tokens[c_i]:\n",
    "                char_tokens_processed.append('<unk>')\n",
    "                char_labels_processed.append('O')\n",
    "                # print(char_tokens_processed)\n",
    "                # print(char_labels_processed)\n",
    "            # except:\n",
    "            #     print(\"unk error\")\n",
    "            #     print(origin_text)\n",
    "            \n",
    "\n",
    "    # for token, label in zip(char_tokens_processed, char_labels_processed):\n",
    "    #     print(f\"token {token} | label : {label}\")\n",
    "    char_tokens = char_tokens_processed\n",
    "    char_labels = char_labels_processed\n",
    "        \n",
    "    #merge char tokens\n",
    "    text_len = len(origin_text)\n",
    "    left = 0\n",
    "    right = 0\n",
    "    char_id = 0 #char_tokens, char_labels 를 스캔하는 id\n",
    "    \n",
    "    try:\n",
    "        while right < text_len: #종료조건 완벽하지 않음\n",
    "            #space가 아닌 char에 대해\n",
    "            if origin_text[right] == ' ':\n",
    "                ner_tagged_sentence += origin_text[right]\n",
    "                right += 1\n",
    "            elif origin_text[right] != ' ':\n",
    "                #항상 origin_text[right] 와 char_tokens[char_id] 가 동일함\n",
    "                if char_labels[char_id].startswith('I-'): #잘못된 예측 I가 먼저 나온 상황\n",
    "                    ner_tagged_sentence += origin_text[right]\n",
    "                    char_id += 1 #개체명으로 인식하지 않고 넘기기\n",
    "                    right += 1\n",
    "\n",
    "                elif char_labels[char_id] == 'O':\n",
    "                    ner_tagged_sentence += origin_text[right]\n",
    "                    char_id += 1\n",
    "                    right += 1\n",
    "\n",
    "                elif char_labels[char_id].startswith('B-'): #여기서는 작업마치고 left, right 저장해야함\n",
    "                    entity = char_labels[char_id][2:]\n",
    "                    pred_label = 'I-'+entity\n",
    "                    left = right\n",
    "                    right += 1\n",
    "                    char_id += 1\n",
    "                    while True:\n",
    "                        if origin_text[right] == ' ':\n",
    "                            right += 1\n",
    "                        elif origin_text[right] != ' ':\n",
    "                            if char_labels[char_id] == pred_label:\n",
    "                                if char_id == len(char_labels)-1:\n",
    "                                    ner_tagged_sentence += (origin_text[left:right+1]+'['+entity+']')\n",
    "                                    ner_tagged_id_list.append([left,right])\n",
    "                                    right += 1\n",
    "                                    break\n",
    "                                else:\n",
    "                                    char_id += 1\n",
    "                                    right += 1\n",
    "                            elif char_labels[char_id] != pred_label:\n",
    "                                if origin_text[right-1] == ' ':\n",
    "                                    ner_tagged_sentence += (origin_text[left:right-1]+'['+entity+']'+' ')\n",
    "                                    ner_tagged_id_list.append([left,right-2])\n",
    "                                else:\n",
    "                                    ner_tagged_sentence += (origin_text[left:right]+'['+entity+']')\n",
    "                                    ner_tagged_id_list.append([left,right-1])\n",
    "                                left = right\n",
    "                                break\n",
    "    except:\n",
    "        print('ERROR')\n",
    "        print(origin_text)\n",
    "        \n",
    "    \n",
    "    # for char_token, char_label in zip(char_tokens, char_labels):\n",
    "    #     print(f\"token : {char_token}, label : {char_label}\")\n",
    "    return ner_tagged_sentence, ner_tagged_id_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "1cb63921-1ffa-47bd-8f37-3fd39eec8b58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "깐쇼새우 만드는 법, 이연복[PER] 셰프 비법은 \"튀김옷 반죽에 식용류\"\n",
      "이연복\n"
     ]
    }
   ],
   "source": [
    "origin_text = '깐쇼새우 만드는 법, 이연복 셰프 비법은 \"튀김옷 반죽에 식용류\"'\n",
    "tokens = ['깐', '쇼', '새우', '만드', '는', '법', ',', '이연', '##복', '셰프', '비법', '은', '<unk>', '튀김', '##옷', '반죽', '에', '식용', '류', '<unk>']\n",
    "labels = ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PER', 'I-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
    "ner_tagged_sentence, ner_tagged_id_list = return_ner_tagged_sentence(origin_text, tokens,labels)\n",
    "print(ner_tagged_sentence)\n",
    "for item in ner_tagged_id_list:\n",
    "    start, end = item\n",
    "    print(origin_text[start:end+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d2a410e5-7f9a-4269-97bf-63dec2dab8b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "신카이 마코토[PER] 감독ㆍ배우 한예리[PER]··· ‘너의 이름은[POH]’ 메가토크[POH]에서 만나요!\n",
      "신카이 마코토\n",
      "한예리\n",
      "너의 이름은\n",
      "메가토크\n"
     ]
    }
   ],
   "source": [
    "origin_text = '신카이 마코토 감독ㆍ배우 한예리··· ‘너의 이름은’ 메가토크에서 만나요!'\n",
    "tokens = ['신', '##카이', '마코토', '감독', '<unk>', '한', '##예', '##리', '·', '·', '·', '<unk>', '너', '의', '이름', '은', '<unk>', '메가', '토크', '에서', '만나', '요', '!']\n",
    "labels = ['B-PER', 'I-PER', 'I-PER', 'O', 'O', 'B-PER', 'I-PER', 'I-PER', 'O', 'O', 'O', 'O', 'B-POH', 'I-POH', 'I-POH', 'I-POH', 'O', 'B-POH', 'I-POH', 'O', 'O', 'O', 'O']\n",
    "ner_tagged_sentence, ner_tagged_id_list = return_ner_tagged_sentence(origin_text, tokens,labels)\n",
    "print(ner_tagged_sentence)\n",
    "for item in ner_tagged_id_list:\n",
    "    start, end = item\n",
    "    print(origin_text[start:end+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "95a1de80-1d67-46fd-bfd1-52ccf5ed55de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "스포츠동아 김민정 기자 ricky337@donga.com 스포츠동아[ORG] 김민정[PER] 기자 ricky337@donga.com[POH]\n",
      "이에 따라 MRO 사업 규모도 같은 기간 643억 달러에서 960억 달러까지 늘어나고 아시아·태평양 지역에서만도 2025년 336억 달러 규모의 시장이 형성될 것으로 예측되고 있다. 이에 따라 MRO 사업 규모도 같은 기간 643억 달러[MNY]에서 960억 달러[MNY]까지 늘어나고 아시아[LOC]·태평양[LOC] 지역에서만도 2025년[DAT] 336억 달러[MNY] 규모의 시장이 형성될 것으로 예측되고 있다.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "answers = []\n",
    "with open(\"test.tsv\", \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = csv.reader(f, delimiter='\\t',)\n",
    "    for i, line in enumerate(lines):\n",
    "        if i>0:\n",
    "            #print(line)\n",
    "            origin_text, _, tokenized_text, label_str = line\n",
    "            tokens = tokenized_text.split()\n",
    "            labels = label_str.split()\n",
    "            #print(origin_text)\n",
    "            #print(tokens)\n",
    "            #print(len(tokens))\n",
    "            #print(labels)\n",
    "            #print(len(labels))\n",
    "            tagged_sentence, tagged_id_list = return_ner_tagged_sentence(origin_text, tokens,labels)\n",
    "            #print(tagged_sentence)\n",
    "            answers.append([origin_text,tagged_sentence])\n",
    "            #print(tagged_id_list)\n",
    "            #print()\n",
    "\n",
    "for i, answer in enumerate(answers):\n",
    "    if i<2:\n",
    "        print(answer[0], answer[1])\n",
    "\n",
    "with open(\"answer.tsv\", \"w\", encoding=\"utf-8\") as f:\n",
    "    field_names = [\"origin_text\", \"ner_tagged_text\"]\n",
    "    writer = csv.writer(f, delimiter='\\t')\n",
    "    writer.writerow(field_names)\n",
    "    for answer in answers:\n",
    "        writer.writerow([answer[0], answer[1]])\n",
    "#            {'origin_text':answer[0], 'ner_tagged_text':answer[1]})\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "53778558-eef5-483e-a7a3-08cd25f263c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token : 스, label : B-ORG\n",
      "token : 포, label : I-ORG\n",
      "token : 츠, label : I-ORG\n",
      "token : 동, label : I-ORG\n",
      "token : 아, label : I-ORG\n",
      "token : 김, label : B-PER\n",
      "token : 민, label : I-PER\n",
      "token : 정, label : I-PER\n",
      "token : 기, label : O\n",
      "token : 자, label : O\n",
      "token : r, label : B-POH\n",
      "token : i, label : I-POH\n",
      "token : c, label : I-POH\n",
      "token : k, label : I-POH\n",
      "token : y, label : I-POH\n",
      "token : 3, label : I-POH\n",
      "token : 3, label : I-POH\n",
      "token : 7, label : I-POH\n",
      "token : @, label : I-POH\n",
      "token : d, label : I-POH\n",
      "token : o, label : I-POH\n",
      "token : n, label : I-POH\n",
      "token : g, label : I-POH\n",
      "token : a, label : I-POH\n",
      "token : ., label : I-POH\n",
      "token : c, label : I-POH\n",
      "token : o, label : I-POH\n",
      "token : m, label : I-POH\n",
      "스포츠동아[ORG] 김민정[PER] 기자 ricky337@donga.com[POH]\n",
      "스포츠동아\n",
      "김민정\n",
      "ricky337@donga.com\n"
     ]
    }
   ],
   "source": [
    "origin_text = '스포츠동아 김민정 기자 ricky337@donga.com'\n",
    "tokens = ['스포츠', '##동', '##아', '김민', '##정', '기자', 'r', '##ick', '##y', '33', '##7', '@', 'do', '##ng', '##a', '.', 'com']\n",
    "labels = ['B-ORG', 'I-ORG', 'I-ORG', 'B-PER', 'I-PER', 'O', 'B-POH', 'I-POH', 'I-POH', 'I-POH', 'I-POH', 'I-POH', 'I-POH', 'I-POH', 'I-POH', 'I-POH', 'I-POH']\n",
    "ner_tagged_sentence, ner_tagged_id_list = return_ner_tagged_sentence(origin_text, tokens,labels)\n",
    "print(ner_tagged_sentence)\n",
    "for item in ner_tagged_id_list:\n",
    "    start, end = item\n",
    "    print(origin_text[start:end+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a38d3d-093b-4f18-8c0a-aa0c7fa8d4ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "43f4d214-4e0c-495c-9595-6ee8da227df5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token : 오, label : O\n",
      "token : 늘, label : O\n",
      "token : 고, label : B-PER\n",
      "token : 보, label : I-PER\n",
      "token : 원, label : I-PER\n",
      "token : 은, label : O\n",
      "token : 9, label : B-TIM\n",
      "token : 시, label : I-TIM\n",
      "token : 에, label : O\n",
      "token : 티, label : B-ORG\n",
      "token : 맥, label : I-ORG\n",
      "token : 스, label : I-ORG\n",
      "token : 로, label : O\n",
      "token : 출, label : O\n",
      "token : 근, label : O\n",
      "token : 했, label : O\n",
      "token : 다, label : O\n",
      "오늘 고보원[PER]은 9시[TIM]에 티맥스[ORG]로 출근했다\n",
      "고보원\n",
      "9시\n",
      "티맥스\n"
     ]
    }
   ],
   "source": [
    "origin_text = '오늘 고보원은 9시에 티맥스로 출근했다'\n",
    "tokens = ['오늘', '고', '##보', '##원', '은', '9', '시', '에', '티', '##맥스', '로', '출근', '했', '다']\n",
    "labels = ['O', 'B-PER', 'I-PER', 'I-PER', 'O', 'B-TIM', 'I-TIM', 'O', 'B-ORG', 'I-ORG', 'O', 'O', 'O', 'O']\n",
    "ner_tagged_sentence, ner_tagged_id_list = return_ner_tagged_sentence(origin_text, tokens,labels)\n",
    "print(ner_tagged_sentence)\n",
    "for item in ner_tagged_id_list:\n",
    "    start, end = item\n",
    "    print(origin_text[start:end+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "af8d5bc5-d055-4e4d-ab2f-89ea82221658",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token : 세, label : O\n",
      "token : 련, label : O\n",
      "token : 미, label : O\n",
      "token : 가, label : O\n",
      "token : 미, label : O\n",
      "token : 한, label : O\n",
      "token : 공, label : O\n",
      "token : 격, label : O\n",
      "token : 우, label : B-ORG\n",
      "token : 리, label : I-ORG\n",
      "token : 은, label : I-ORG\n",
      "token : 행, label : I-ORG\n",
      "token : 박, label : B-PER\n",
      "token : 혜, label : I-PER\n",
      "token : 진, label : I-PER\n",
      "token : ,, label : O\n",
      "token : 임, label : B-PER\n",
      "token : 영, label : I-PER\n",
      "token : 희, label : I-PER\n",
      "token : ,, label : O\n",
      "token : 양, label : B-PER\n",
      "token : 지, label : I-PER\n",
      "token : 희, label : I-PER\n",
      "token : 는, label : O\n",
      "token : 공, label : O\n",
      "token : 격, label : O\n",
      "token : 에, label : O\n",
      "token : 도, label : O\n",
      "token : 능, label : O\n",
      "token : 하, label : O\n",
      "token : 다, label : O\n",
      "token : ., label : O\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('세련미 가미한 공격 우리은행[ORG] 박혜진[PER], 임영희[PER], 양지희[PER]는 공격에도 능하다.',\n",
       " [[11, 14], [16, 18], [21, 23], [26, 28]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "origin_text = \"세련미 가미한 공격 우리은행 박혜진, 임영희, 양지희는 공격에도 능하다.\"\n",
    "tokens = ['세련', '##미', '가', '미', '한', '공격', '우리', '##은행', '박', '##혜진', ',', '임영', '##희', ',', '양지', '##희', '는', '공격', '에', '도', '능하', '다', '.']\n",
    "labels = ['O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'I-ORG', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
    "return_ner_tagged_sentence(origin_text, tokens,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03182e5-afe1-4960-8d87-3529d64ade9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting konlpy\n",
      "  Using cached konlpy-0.5.2-py2.py3-none-any.whl (19.4 MB)\n",
      "Requirement already satisfied: numpy>=1.6 in /home/ubuntu/anaconda3/envs/pretrain/lib/python3.7/site-packages (from konlpy) (1.21.4)\n",
      "Collecting tweepy>=3.7.0\n",
      "  Using cached tweepy-4.4.0-py2.py3-none-any.whl (65 kB)\n",
      "Collecting JPype1>=0.7.0\n",
      "  Using cached JPype1-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (448 kB)\n",
      "Collecting beautifulsoup4==4.6.0\n",
      "  Using cached beautifulsoup4-4.6.0-py3-none-any.whl (86 kB)\n",
      "Collecting colorama\n",
      "  Using cached colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n",
      "Collecting lxml>=4.1.0\n",
      "  Downloading lxml-4.7.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (6.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 6.4 MB 9.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions in /home/ubuntu/anaconda3/envs/pretrain/lib/python3.7/site-packages (from JPype1>=0.7.0->konlpy) (3.10.0.2)\n",
      "Requirement already satisfied: requests<3,>=2.11.1 in /home/ubuntu/anaconda3/envs/pretrain/lib/python3.7/site-packages (from tweepy>=3.7.0->konlpy) (2.26.0)\n",
      "Collecting requests-oauthlib<2,>=1.0.0\n",
      "  Using cached requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ubuntu/anaconda3/envs/pretrain/lib/python3.7/site-packages (from requests<3,>=2.11.1->tweepy>=3.7.0->konlpy) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/ubuntu/anaconda3/envs/pretrain/lib/python3.7/site-packages (from requests<3,>=2.11.1->tweepy>=3.7.0->konlpy) (2.0.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ubuntu/anaconda3/envs/pretrain/lib/python3.7/site-packages (from requests<3,>=2.11.1->tweepy>=3.7.0->konlpy) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ubuntu/anaconda3/envs/pretrain/lib/python3.7/site-packages (from requests<3,>=2.11.1->tweepy>=3.7.0->konlpy) (1.26.7)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Using cached oauthlib-3.1.1-py2.py3-none-any.whl (146 kB)\n",
      "Installing collected packages: oauthlib, requests-oauthlib, tweepy, lxml, JPype1, colorama, beautifulsoup4, konlpy\n",
      "Successfully installed JPype1-1.3.0 beautifulsoup4-4.6.0 colorama-0.4.4 konlpy-0.5.2 lxml-4.7.1 oauthlib-3.1.1 requests-oauthlib-1.3.0 tweepy-4.4.0\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
