{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "bddaa297-5211-4215-86f3-ad943d120b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_ner_tagged_sentence(origin_text, tokens, labels):\n",
    "    ner_tagged_sentence = ''\n",
    "    ner_tagged_id_list = []\n",
    "    \n",
    "    #divide label per character token\n",
    "    #space를 제외한 모든 글자에 대한 라벨을 붙이기\n",
    "    #abc : B-LOC → a : B-LOC, b : I-LOC, c : I-LOC\n",
    "    char_labels = []\n",
    "    char_tokens = []\n",
    "    for token, label in zip(tokens, labels):\n",
    "        if token == '<unk>': #이 토큰에 대한 글자수를 알 수 있나? 그럼 끝나는데\n",
    "            if label.startswith(\"B-\"):\n",
    "                print(\"unk인데 태깅되어 있는 경우 발생\")\n",
    "                print(origin_text)\n",
    "                print(token, label)\n",
    "            # char_tokens.append('<unk>')\n",
    "            # char_labels.append('O')\n",
    "            continue\n",
    "            \n",
    "        if label == 'O':\n",
    "            for i in range(len(token)):\n",
    "                #한글자씩 분해시킨 토큰\n",
    "                char_token = token[i]\n",
    "                char_label = 'O'\n",
    "                if char_token != \"#\":\n",
    "                    char_labels.append(char_label)\n",
    "                    char_tokens.append(char_token)\n",
    "        elif label.startswith(\"B-\"):\n",
    "            entity = label[2:]\n",
    "            for i in range(len(token)):\n",
    "                char_token = token[i]\n",
    "                if char_token != \"#\":\n",
    "                    if i == 0:\n",
    "                        char_label = \"B-\"+ entity\n",
    "                    elif i > 0:\n",
    "                        char_label = \"I-\"+ entity\n",
    "                elif char_token == \"#\":\n",
    "                    if i < 2:\n",
    "                        continue\n",
    "                    elif i == 2:\n",
    "                        char_label = \"B-\"+ entity\n",
    "                    elif i > 2:\n",
    "                        char_label = \"I-\"+ entity\n",
    "                        \n",
    "                char_labels.append(char_label)\n",
    "                char_tokens.append(char_token)\n",
    "        elif label.startswith(\"I-\"):\n",
    "            entity = label[2:]\n",
    "            for i in range(len(token)):\n",
    "                char_token = token[i]\n",
    "                char_label = \"I-\"+entity\n",
    "                if char_token != \"#\":\n",
    "                    char_tokens.append(char_token)\n",
    "                    char_labels.append(char_label)\n",
    "\n",
    "    #process for <unk>\n",
    "    char_tokens_processed = []\n",
    "    char_labels_processed = []\n",
    "    c_i = 0\n",
    "    for i in range(len(origin_text)):\n",
    "        #space가 아닌 글자에 대해서\n",
    "        origin_char = origin_text[i]\n",
    "        if origin_char != ' ':\n",
    "            #is_unk = True\n",
    "            # try:\n",
    "            if c_i == len(char_tokens):\n",
    "                char_tokens_processed.append('<unk>')\n",
    "                char_labels_processed.append('O')\n",
    "                # print(char_tokens_processed)\n",
    "                # print(char_labels_processed)\n",
    "                continue\n",
    "\n",
    "            if origin_char == char_tokens[c_i]:\n",
    "                #is_unk = False #해당 origin_char는 unk이 아님\n",
    "                char_tokens_processed.append(char_tokens[c_i])\n",
    "                char_labels_processed.append(char_labels[c_i])\n",
    "                #if c_i < len(char_tokens)-1:\n",
    "                c_i += 1\n",
    "                # print(char_tokens_processed)\n",
    "                # print(char_labels_processed)\n",
    "                \n",
    "\n",
    "            elif origin_char != char_tokens[c_i]:\n",
    "                char_tokens_processed.append('<unk>')\n",
    "                char_labels_processed.append('O')\n",
    "                # print(char_tokens_processed)\n",
    "                # print(char_labels_processed)\n",
    "            # except:\n",
    "            #     print(\"unk error\")\n",
    "            #     print(origin_text)\n",
    "            \n",
    "\n",
    "    # for token, label in zip(char_tokens_processed, char_labels_processed):\n",
    "    #     print(f\"token {token} | label : {label}\")\n",
    "    char_tokens = char_tokens_processed\n",
    "    char_labels = char_labels_processed\n",
    "        \n",
    "    #merge char tokens\n",
    "    text_len = len(origin_text)\n",
    "    left = 0\n",
    "    right = 0\n",
    "    char_id = 0 #char_tokens, char_labels 를 스캔하는 id\n",
    "    \n",
    "    try:\n",
    "        while right < text_len: #종료조건 완벽하지 않음\n",
    "            #space가 아닌 char에 대해\n",
    "            if origin_text[right] == ' ':\n",
    "                ner_tagged_sentence += origin_text[right]\n",
    "                right += 1\n",
    "            elif origin_text[right] != ' ':\n",
    "                #항상 origin_text[right] 와 char_tokens[char_id] 가 동일함\n",
    "                if char_labels[char_id].startswith('I-'): #잘못된 예측 I가 먼저 나온 상황\n",
    "                    ner_tagged_sentence += origin_text[right]\n",
    "                    char_id += 1 #개체명으로 인식하지 않고 넘기기\n",
    "                    right += 1\n",
    "\n",
    "                elif char_labels[char_id] == 'O':\n",
    "                    ner_tagged_sentence += origin_text[right]\n",
    "                    char_id += 1\n",
    "                    right += 1\n",
    "\n",
    "                elif char_labels[char_id].startswith('B-'): #여기서는 작업마치고 left, right 저장해야함\n",
    "                    entity = char_labels[char_id][2:]\n",
    "                    pred_label = 'I-'+entity\n",
    "                    left = right\n",
    "                    right += 1\n",
    "                    char_id += 1\n",
    "                    while True:\n",
    "                        if origin_text[right] == ' ':\n",
    "                            right += 1\n",
    "                        elif origin_text[right] != ' ':\n",
    "                            if char_labels[char_id] == pred_label:\n",
    "                                if char_id == len(char_labels)-1:\n",
    "                                    ner_tagged_sentence += (origin_text[left:right+1]+'['+entity+']')\n",
    "                                    ner_tagged_id_list.append([left,right])\n",
    "                                    right += 1\n",
    "                                    break\n",
    "                                else:\n",
    "                                    char_id += 1\n",
    "                                    right += 1\n",
    "                            elif char_labels[char_id] != pred_label:\n",
    "                                if origin_text[right-1] == ' ':\n",
    "                                    ner_tagged_sentence += (origin_text[left:right-1]+'['+entity+']'+' ')\n",
    "                                    ner_tagged_id_list.append([left,right-2])\n",
    "                                else:\n",
    "                                    ner_tagged_sentence += (origin_text[left:right]+'['+entity+']')\n",
    "                                    ner_tagged_id_list.append([left,right-1])\n",
    "                                left = right\n",
    "                                break\n",
    "    except:\n",
    "        print('ERROR')\n",
    "        print(origin_text)\n",
    "        \n",
    "    \n",
    "    # for char_token, char_label in zip(char_tokens, char_labels):\n",
    "    #     print(f\"token : {char_token}, label : {char_label}\")\n",
    "    return ner_tagged_sentence, ner_tagged_id_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "98492d06-ab62-4d77-b46e-c2741ed30ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "origin_text : 입력 문장\n",
    "origin_tokens : tokenization 과정을 거쳤지만 <unk>인 경우에도 <unk>으로 변경 안 한 토큰\n",
    "tokens : tokenization 과정을 거치고 <unk>인 경우를 <unk>으로 변경한 토큰들 (이전)\n",
    "labels : origin_tokens, tokens에 대한 라벨\n",
    "'''\n",
    "def return_ner_tagged_sentence_plus(origin_text, origin_tokens, labels):\n",
    "    ner_tagged_sentence = ''\n",
    "    ner_tagged_id_list = []\n",
    "    \n",
    "    #divide label per character token\n",
    "    #space를 제외한 모든 글자에 대한 라벨을 붙이기\n",
    "    #abc : B-LOC → a : B-LOC, b : I-LOC, c : I-LOC\n",
    "    char_labels = []\n",
    "    char_tokens = []\n",
    "    for token, label in zip(origin_tokens, labels):\n",
    "        if label == 'O':\n",
    "            for i in range(len(token)):\n",
    "                #한글자씩 분해시킨 토큰\n",
    "                char_token = token[i]\n",
    "                char_label = 'O'\n",
    "                if char_token != \"#\":\n",
    "                    char_labels.append(char_label)\n",
    "                    char_tokens.append(char_token)\n",
    "        elif label.startswith(\"B-\"):\n",
    "            entity = label[2:]\n",
    "            for i in range(len(token)):\n",
    "                char_token = token[i]\n",
    "                if char_token != \"#\":\n",
    "                    if i == 0:\n",
    "                        char_label = \"B-\"+ entity\n",
    "                    elif i > 0:\n",
    "                        char_label = \"I-\"+ entity\n",
    "                elif char_token == \"#\":\n",
    "                    if i < 2:\n",
    "                        continue\n",
    "                    elif i == 2:\n",
    "                        char_label = \"B-\"+ entity\n",
    "                    elif i > 2:\n",
    "                        char_label = \"I-\"+ entity\n",
    "                        \n",
    "                char_labels.append(char_label)\n",
    "                char_tokens.append(char_token)\n",
    "        elif label.startswith(\"I-\"):\n",
    "            entity = label[2:]\n",
    "            for i in range(len(token)):\n",
    "                char_token = token[i]\n",
    "                char_label = \"I-\"+entity\n",
    "                if char_token != \"#\":\n",
    "                    char_tokens.append(char_token)\n",
    "                    char_labels.append(char_label)\n",
    "        \n",
    "    #merge char tokens\n",
    "    text_len = len(origin_text)\n",
    "    left = 0\n",
    "    right = 0\n",
    "    char_id = 0 #char_tokens, char_labels 를 스캔하는 id\n",
    "    \n",
    "    try:\n",
    "        while right < text_len: #종료조건 \n",
    "            #space가 아닌 char에 대해\n",
    "            if origin_text[right] == ' ':\n",
    "                ner_tagged_sentence += origin_text[right]\n",
    "                right += 1\n",
    "            elif origin_text[right] != ' ':\n",
    "                #항상 origin_text[right] 와 char_tokens[char_id] 가 동일함\n",
    "                if char_labels[char_id].startswith('I-'): #잘못된 예측 I가 먼저 나온 상황\n",
    "                    ner_tagged_sentence += origin_text[right]\n",
    "                    char_id += 1 #개체명으로 인식하지 않고 넘기기\n",
    "                    right += 1\n",
    "\n",
    "                elif char_labels[char_id] == 'O':\n",
    "                    ner_tagged_sentence += origin_text[right]\n",
    "                    char_id += 1\n",
    "                    right += 1\n",
    "\n",
    "                elif char_labels[char_id].startswith('B-'): #여기서는 작업마치고 left, right 저장해야함\n",
    "                    entity = char_labels[char_id][2:]\n",
    "                    pred_label = 'I-'+entity\n",
    "                    left = right\n",
    "                    right += 1\n",
    "                    char_id += 1\n",
    "                    while True:\n",
    "                        if origin_text[right] == ' ':\n",
    "                            right += 1\n",
    "                        elif origin_text[right] != ' ':\n",
    "                            if char_labels[char_id] == pred_label:\n",
    "                                if char_id == len(char_labels)-1:\n",
    "                                    ner_tagged_sentence += (origin_text[left:right+1]+'['+entity+']')\n",
    "                                    ner_tagged_id_list.append([left,right,entity])\n",
    "                                    right += 1\n",
    "                                    break\n",
    "                                else:\n",
    "                                    char_id += 1\n",
    "                                    right += 1\n",
    "                            elif char_labels[char_id] != pred_label:\n",
    "                                if origin_text[right-1] == ' ':\n",
    "                                    ner_tagged_sentence += (origin_text[left:right-1]+'['+entity+']'+' ')\n",
    "                                    ner_tagged_id_list.append([left,right-2,entity])\n",
    "                                else:\n",
    "                                    ner_tagged_sentence += (origin_text[left:right]+'['+entity+']')\n",
    "                                    ner_tagged_id_list.append([left,right-1,entity])\n",
    "                                left = right\n",
    "                                break\n",
    "    except:\n",
    "        print('ERROR')\n",
    "        print(origin_text)\n",
    "        \n",
    "    \n",
    "    # for char_token, char_label in zip(char_tokens, char_labels):\n",
    "    #     print(f\"token : {char_token}, label : {char_label}\")\n",
    "    return ner_tagged_sentence, ner_tagged_id_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "1cb63921-1ffa-47bd-8f37-3fd39eec8b58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "깐쇼새우 만드는 법, 이연복[PER] 셰프 비법은 \"튀김옷 반죽에 식용류\"\n",
      "이연복\n"
     ]
    }
   ],
   "source": [
    "origin_text = '깐쇼새우 만드는 법, 이연복 셰프 비법은 \"튀김옷 반죽에 식용류\"'\n",
    "tokens = ['깐', '쇼', '새우', '만드', '는', '법', ',', '이연', '##복', '셰프', '비법', '은', '<unk>', '튀김', '##옷', '반죽', '에', '식용', '류', '<unk>']\n",
    "labels = ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PER', 'I-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
    "ner_tagged_sentence, ner_tagged_id_list = return_ner_tagged_sentence(origin_text, tokens,labels)\n",
    "print(ner_tagged_sentence)\n",
    "for item in ner_tagged_id_list:\n",
    "    start, end = item\n",
    "    print(origin_text[start:end+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d2a410e5-7f9a-4269-97bf-63dec2dab8b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "신카이 마코토[PER] 감독ㆍ배우 한예리[PER]··· ‘너의 이름은[POH]’ 메가토크[POH]에서 만나요!\n",
      "신카이 마코토\n",
      "한예리\n",
      "너의 이름은\n",
      "메가토크\n"
     ]
    }
   ],
   "source": [
    "origin_text = '신카이 마코토 감독ㆍ배우 한예리··· ‘너의 이름은’ 메가토크에서 만나요!'\n",
    "tokens = ['신', '##카이', '마코토', '감독', '<unk>', '한', '##예', '##리', '·', '·', '·', '<unk>', '너', '의', '이름', '은', '<unk>', '메가', '토크', '에서', '만나', '요', '!']\n",
    "labels = ['B-PER', 'I-PER', 'I-PER', 'O', 'O', 'B-PER', 'I-PER', 'I-PER', 'O', 'O', 'O', 'O', 'B-POH', 'I-POH', 'I-POH', 'I-POH', 'O', 'B-POH', 'I-POH', 'O', 'O', 'O', 'O']\n",
    "ner_tagged_sentence, ner_tagged_id_list = return_ner_tagged_sentence(origin_text, tokens,labels)\n",
    "print(ner_tagged_sentence)\n",
    "for item in ner_tagged_id_list:\n",
    "    start, end = item\n",
    "    print(origin_text[start:end+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "bd7c196a-e123-490b-b6a3-308dc8b1551f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/bowon_ko\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'posixpath' has no attribute 'dir'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_85020/3944186340.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"data\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"test.tsv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: module 'posixpath' has no attribute 'dir'"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())\n",
    "os.path.join(os.path.dir(os.getcwd())+\"data\",\"test.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "95a1de80-1d67-46fd-bfd1-52ccf5ed55de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR\n",
      "지난 14일 방송된 KBS 2TV '불후의 명곡' 왕중왕전에서는 '쇼쇼쇼, 별들의 귀환' 2편이 꾸며졌다.\n",
      "ERROR\n",
      "사진·글 = 임현동 기자\n",
      "ERROR\n",
      "우진원 디자이너는 허벅지부터 발끝까지 지퍼가 길게 달린 통 넓은 청바지와 발목까지 내려오는 길이의 통 넓은 청치마를 ‘로켓런치’ 패션쇼에 올렸다.\n",
      "ERROR\n",
      "유승호와 솔라의 달콤한 입맞춤을 예고한 ‘상상고양이’ 2회는 1일 오후 8시 50분에 방송된다.\n",
      "ERROR\n",
      "새누리 무리는 새로운 세상을 연다는 의미에서 '개(開)'자를 써서 개누리라 불리며 공주를 수호하는 데 여념하였다.\n",
      "ERROR\n",
      "순끼 작가의 동명 웹툰을 원자으로 한 '치즈인더트랩'은 모든 것이 완벽한 남자 유정(박해진 분)과 평범하지만 예민한 그의 대학 후배 홍설(오연서 분), 그리고 백인호(박기웅 분)등이 대학 캠퍼스에서 벌어지는 일을 그린 작품. 산다라박, 오종혁, 문지윤 등도 출연한다.\n",
      "ERROR\n",
      "\"지하 3층에 거꾸로 파묻혔지만…\" 강남 건물붕괴 인부2명 전원구조\n",
      "ERROR\n",
      "손연재 개인종합 사상 첫 우승… 72.55점, 4종목 모두 18점 넘어\n",
      "ERROR\n",
      "손흥민 골, ‘400억 가치 증명’ 3경기면 충분했다…\n",
      "ERROR\n",
      "대표 상품은 올 초 교보생명이 VIP 수준을 넘어 ‘VVIP’ 전용으로 내놓은 ‘교보노블리에종신보험’이다.\n",
      "ERROR\n",
      "마르셀 회장은 “서울시와 2년간 협의한 끝에 20년 동안 임대하게 됐다”며 “80년 가까이 된 건물 내부를 리모델링하고 최신 음향·조명 시설을 설치하는 등 총 190억원을 투자했다”고 밝혔다.\n",
      "ERROR\n",
      "■ 동탄2신도시 등 뉴스테이 사업자 공모국토교통부와 주택도시보증공사는 경기 화성시 동탄2신도시와 경기 수원시 호매실지구에 기업형임대주택(뉴스테이)을 공급할 사업자를 21일 공모한다고 20일 밝혔다.\n",
      "ERROR\n",
      "‘폭염주의보 발효 태풍 할롤라 소멸’제12호 태풍 ‘할롤라’가 27일 오전 0시를 기해 소멸한 가운데, 부산 경남과 강원 동해안에 폭염주의보가 내려졌다.\n",
      "ERROR\n",
      "김구라는 다시 “그럼 박원순 시장이 낫다는거냐”고 말했고 강용석은 “그렇다,\n",
      "ERROR\n",
      "대국 시간이 4시간을 넘길 즈음부터 “덤내기가 쉽지 않다”란 전망이 나왔고, 그건 한 시간 가까이 지속됐다.\n",
      "ERROR\n",
      "‘업무 재부팅(Rebooting Work)’의 저자 메이너드웹은 이렇게 말한다.\n",
      "ERROR\n",
      "‘줴이’ 딥러닝 할 때 ‘돌바람’ 개발자는 생계용 SW 개발\n",
      "ERROR\n",
      "오른손으로 최고 시속이 145㎞ 정도 나오지만 왼손으로는 135㎞ 정도다.\n",
      "ERROR\n",
      "‘힐링캠프’ 유준상, “ 이 방송은 유재석에게 바치는 방송” 왜?\n",
      "ERROR\n",
      "양현종 개막전 선발 등판…'메이저리그행 무산' 恨 풀까\n",
      "ERROR\n",
      "술을 마시고도 운전을 하는 가장 큰 이유로는 “술을 많이 먹지 않아 이상이 없다고 느껴서”라는 답변이 22.7%(34명)로 가장 많았다.\n",
      "ERROR\n",
      "배상문 입대 “죄송하다”, 2017년 9월 PGA 투어로 복귀\n",
      "ERROR\n",
      "<이하 박정아 소속사 전문>안녕하세요. 젤리피쉬엔터테인먼트입니다.\n",
      "ERROR\n",
      "기부를 시작한 2006년 한 해에 전달한 신발은 1만 켤레. 탐스는 ‘착한 신발’로 알려지면서 창립 8년 만에 신발 총 3500만 켤레를 판매했다.\n",
      "ERROR\n",
      "‘전보다 좋아졌다’는 응답은 심 후보가 52%로 가장 많았고 유 후보가 33%로 그 뒤를 이었다.\n",
      "ERROR\n",
      "김지우·레이먼킴 신혼집, 딸 김루아나리 방송 최초 공개…이름의 뜻은?레이먼킴·김지우 부부가 '택시'에서 신혼집을 공개하며 딸 김루아나리를 방송 최초 공개했다.\n",
      "ERROR\n",
      "최민식은 “170억이 넘는 제작비에 200억 가까운 제작비를 사용한 '라이언킹'이 되지 않으려면, CG라는 생각조차도 없어질 만큼 드라마가 굉장히 중요하다고 느꼈다”며 “그래서 천만덕의 가치관, 세계관 생을 살아가는 천만덕의 태도에 집중할 수밖에 없었다”고 밝혔다.\n",
      "ERROR\n",
      "한편 ‘그녀는 예뻤다’는 수-목 오후 10시에 방송한다.\n",
      "ERROR\n",
      "안세하, 상경해 배우 된 계기? “2년 동안 가수 되기 위해 연습”배우 안세하가 ‘나 혼자 산다’\n",
      "ERROR\n",
      "[Money&Life]SK텔레콤과 손잡고 대출상품 선봬\n",
      "ERROR\n",
      "KBS2 ‘슈퍼맨이 돌아왔다’\n",
      "ERROR\n",
      "‘자유청년연합’이란 단체는 스마트폰 카메라를 들고 구치소 현장을 유튜브로 생중계했다.\n",
      "ERROR\n",
      "또한 '라스' 초아는 “한 달에 500만원을 벌었다.\n",
      "ERROR\n",
      "‘베나치오’는 이경규 효과 덕분인지 지난해 한 해 판매량 1000만병을 돌파했다.\n",
      "ERROR\n",
      "장상현 ㈜생각투자 팀장은 보드게임 ‘승경도’를 추천했다.\n",
      "ERROR\n",
      "다양한 얼굴 표정을 통해 감정을 자유자재로 표현하는 ‘메로3’(사진)와 이족보행 로봇인 ‘키보’가 관람객을 잡아끈다.\n",
      "ERROR\n",
      "쯔위 ‘대만 국기’ 논란에 JYP 홈페이지 다운…‘어나니머스’ 대만 해커들 공격 추정그룹 트와이스 쯔위의 대만 국기 논란에 소속사 JYP엔터테인먼트의 홈페이지까지 다운됐다.\n",
      "ERROR\n",
      "앞서 그녀는 ‘아이러니’ 무대를 통해 현장 팬투표 1위에 오른 바 있다.\n",
      "ERROR\n",
      "민족문제연구소는 24일 태평양전쟁피해보상추진협의회와 독립영화제작소 `스피리통' 등 일본 시민단체 등과 함께 `2005 한일 공동 다큐멘터리 제작위원회'를 구성, 다큐멘터리 `안녕, 사요나라'(가제)를 제작중이라고 밝혔다.\n",
      "ERROR\n",
      "장첸이 30일부터 한국 팬과 만나게 되는 영화 '에로스'는 홍콩의 왕자웨이, 미국의 스티븐 소더버그, 이탈리아의 미켈란젤로 안토니오니 등 세 거장이 각각 '에로스'(Eros)를 주제로 만든 중편을 묶은 옴니버스 영화. 장첸은 왕자웨이의 작품 '그녀의 손길'에서 궁리(鞏悧)와 호흡을 맞췄다.\n",
      "ERROR\n",
      "XTM, '올빼미 영화제' 개최\n",
      "ERROR\n",
      "응모자는 9월 23일까지 ▲시놉시스 ▲시나리오 ▲제작기획서 ▲감독 필모그러피 ▲포트폴리오 등을 사무국에 접수하면 된다. 대상자에게는 상금 500만원과 미주 왕복항공권 2매가 시상되며 이 제도를 거쳐 완성된 영화는 내년도 영화제의 개막작으로 선보인다. ☎02-783-6518~9, www.aisff.org 제3회 아시아나국제단편영화제는 11월2-7일 서울에서 열리며 영화제 직후부터 6개월 동안 아시아나 항공에서 기내 고객들을 대상으로 상영회를 갖는다.\n",
      "ERROR\n",
      "▲'우주전쟁'의 제작비는 약 1억3천만 달러(약 1천340억원)로 29일 개봉 첫날 미국과 캐나다에서만 2천130만 달러(약 220억원)를 벌어들였다.\n",
      "ERROR\n",
      "지난 4-5월 2005 전주국제영화제에서 개막작으로 상영된 바 있는 '디지털3인3색'이 22일 서울 종로구 낙원동의 필름포럼(구 허리우드극장)에서 개봉한다.\n",
      "ERROR\n",
      "지난해 업무보고에서 한부총리가 밝힌 대북정책 추진의 기조는 「민족복리우선」과 「북한의 고립­봉쇄 반대」였다.\n",
      "ERROR\n",
      "유성열 ryu@donga.com / 세종=홍수용 기자\n",
      "ERROR\n",
      "한편 차승원과 유해진은 1999년 영화 ‘주유소 습격 사건’을 인연으로 2006년 영화 ‘국경의 남쪽’에서 주연과 조연으로, 2007년 영화 ‘이장과 군수’에선 두 주연으로 본격 콤비로 활약한바 있다.\n",
      "ERROR\n",
      "개인연금제 5월 시행/재무부 발표/은행­보험에 10년이상 불입\n",
      "ERROR\n",
      "‘대전 메르스 의심환자 사망’대전에서 메르스 의심환자로 분류된 80대가 사망했다.\n",
      "ERROR\n",
      "어린이그룹 '7공주'가 여성가족부 출범 기념 캠페인송 '가족사랑 행복송-가족'을 불렀다.\n",
      "ERROR\n",
      "언프리티랩스타 시즌2 전지윤 랩에 수아 '듣는 순간 이게 뭐지 싶었다'라고 평가…‘언프리티랩스타 시즌2’ 전지윤이 아이돌 래퍼에 대한 편견에 일침을 가했다.\n",
      "ERROR\n",
      "여신들의 경쟁, 김태희 ‘용팔이’ vs 송혜교 ‘태양의 후예’\n",
      "ERROR\n",
      "‘이민호 열애 언급’ 수지, 최근 SNS글 보니 의미심장? “고마워요. 믿어줘서”수지 이민호 열애 언급그룹 미쓰에이 수지가 ‘해피투게더’에서 이민호와의 열애 사실에 대해 언급한 가운데, 수지의 최근 SNS글이 눈길을 모았다.수지는 2일 자신의 인스타그램에 “꺼억~ 잘먹었습니다!\n",
      "ERROR\n",
      "한편 이날 'CCTV-MTV 만다린 뮤직 아너스 2005'에서는 중국서 활동중인 장나라가 '올해의 한국 가수상' 시상자로 나서 눈길을 끌었다. '올해의 한국 가수상'은 중국 MTV에서 올 한해 동안 방송된 뮤직비디오 횟수가 가장 많은 아티스트를 선정해 시상한다. 지난 '올해의 한국 가수상'은 신승훈(6회), 장나라(5회), jtL(4회)이 수상했다. 이날 비는 '나쁜 남자', 'It's Raining'으로 시상식의 피날레를 장식했다. 이 시상식은 8월 27일 오후 5시 MTV 코리아에서 녹화 중계된다.\n",
      "ERROR\n",
      "우리은행, 3연승으로 ‘통합 5연패’ 달성\n",
      "ERROR\n",
      "[미리보는 오늘] 점입가경 이화여대 갈등, 출구 찾을까\n",
      "ERROR\n",
      "오후에는 중국의 최대 포털인 바이두(百度)를 방문해 바이보(白伯) 부사장 등을 만났다.\n",
      "ERROR\n",
      "'축구황제' 펠레가 '제2의 펠레'란 찬사를 듣고 있는 브라질 축구 유망주 호비뉴(21.산토스)의 유럽행을 지지하고 나섰다.\n",
      "ERROR\n",
      "\"2년만에 다시 찾은 한국 땅에서 피스컵 2연패의 기쁨을 맛보고 싶다.\"(거스 히딩크 PSV에인트호벤 감독)\n",
      "ERROR\n",
      "'아주리 군단'의 신예 골잡이 알베르토 질라르디노(23.전 파르마)가 이탈리아 프로축구 세리에A의 명문 AC밀란으로 이적했다.\n",
      "ERROR\n",
      "'킥 앤드 러시'의 정통 스타일을 구사한 토튼햄의 창이 프랑스 대표 미드필더진이 대거 포진한 리옹의 방패를 뚫었다.\n",
      "ERROR\n",
      "온라인 중앙일보[사진 YNB 엔터테인먼트]\n",
      "ERROR\n",
      "본프레레 감독, \"박주영 출전여부는 29일 결정\"\n",
      "ERROR\n",
      "지난 3일 방송된 ‘해피투게더3’에는 ‘미쿡에서 왔어요’ 특집으로 이현우, 윤상, 존 박, 에릭 남, 스테파니 리가 출연했다.\n",
      "ERROR\n",
      "포우 가솔은 18점 10리바운드로 브라이언트와 나란히 '더블더블'을 기록했고 식스맨 루크 월튼은 18점 7리바운드 5어시스트를 올렸다 .\n",
      "ERROR\n",
      "채수빈, 구자욱과 열애설 부인…연인도 아닌데 손잡고? 소속사 “오해살 만 하지만…”배우 채수빈이 프로야구 삼성 라이온즈의 구자욱과의 열애설을 부인했다.\n",
      "ERROR\n",
      "우리은행, 3연승으로 ‘통합 5연패’ 달성\n",
      "ERROR\n",
      "이어 △임진각(파주·580만 명) △킨텍스(고양·521만8000명) △서울대공원(과천·470만6000명) 등이었다.\n",
      "ERROR\n",
      "”-얼마나 먹어야 건강과 노화 방지에 도움이 되는지.“레스베라트롤을 예로 들면 적포도주에 함유된 양으로 신체에 직접 도움을 주려면 하루 600잔은 마셔야 한다.\n",
      "ERROR\n",
      "더불어 ‘위 라이크 투 파티’는 ‘뱅뱅뱅’에 이어 8개 음원사이트 2위에 오르며 빅뱅의 위력을 입증했다.\n",
      "ERROR\n",
      "지난달 27일 방송된 MBC ‘일밤-복면가왕’에서는 12대 가왕 ‘사랑은 연필로 쓰세요’와 도전자 ‘소녀의 순정 코스모스’가 대결하는 장면이 그려졌다.\n",
      "ERROR\n",
      "기아자동차의 올해 주력모델인 ‘신형 K5’의 가격대가 2235만~3145만 원으로 결정됐다.\n",
      "ERROR\n",
      "●올림픽 국가대표 2차선발전(태릉선수촌 양궁장)\n",
      "ERROR\n",
      "석명준(LG) - 3년간 평균연봉 1억원 (인센티브 2000만원)이중원(KCC) - 4년간 평균연봉 5000만원 (인센티브 2000만원)김기만(SK) - 4년간 평균연봉 1억3200만원 (인센티브 800만원)김종학(SK) - 4년간 평균연봉 8500만원 (인센티브 500만원)김병철(오리온스) - 2년간 평균연봉 2억2000만원 (인센티브 2000만원)이현준(오리온스) - 5년간 평균연봉 1억3000만원 (인센티브 2000만원)오용준(오리온스) - 3년간 평균연봉 1억7000만원이창수(모비스) - 1년간 평균연봉 1억1000만원 (인센티브 1500만원)우지원(모비스) - 2년간 평균연봉 2억1000만원 (인센티브 3000만원)강우형(모비스) - 1년간 평균연봉 3500만원 (인센티브 500만원)변청운(동부) - 2년간 평균연봉 8500만원김진호(동부) - 3년간 평균연봉 6000만원임영훈(KTF) - 3년간 평균연봉 7500만원 (인센티브 500만원)[모비스 우지원 .\n",
      "ERROR\n",
      "'야생마' 양용은(36 ㆍ 테일러메이드 ㆍ 사진)이 미국프로골프(PGA)투어 AT&T클래식(총상금 550만달러) 1라운드에서 공동 40위에 올랐다 .\n",
      "ERROR\n",
      "[뉴스플러스] 최무배, 스피릿MC 대회 출전 확정 外\n",
      "ERROR\n",
      "전체 101만명 중 9642명만 이용… 시스템 개발 행자부도 145명 그쳐\n",
      "ERROR\n",
      "‘5·31 교육개혁 그리고 20년’ 책 낸 안병영 前 교육부장관\n",
      "ERROR\n",
      "이날 김소연은 수줍은 소녀 같은 제스처와 “빨리 안건~”이라는 애교 섞인 재촉으로 G12를 열광케 했다.\n",
      "ERROR\n",
      "☞ 관련기사 보기 배유나-양효진 \"일본전 두렵지 않다\"\n",
      "ERROR\n",
      "지난해 삼성서울병원 정신건강의학과에서 발표한 내용을 보면 일주일 기준으로 대기 중 미세먼지 농도가 1㎥당 37.8㎍이 증가하면 자살률이 3.2%씩 증가한다.\n",
      "ERROR\n",
      "‘역전의 여왕’ 신지애…JLPGA 히구치 히사코 레이디스 우승\n",
      "ERROR\n",
      "●삼성챔피언스트로피(독일 뮌헨글라드바흐)\n",
      "ERROR\n",
      "[단독]박원오 작년 8월 최순실에게 “삼성서 계약 빨리 하자고 한다”\n",
      "ERROR\n",
      "'빅스타' 가운데서는 이 대회에서 두차례(1994년, 1997년)나 우승했던 '황태자' 어니 엘스(남아공)가 공동 9위(이븐파 142타)에 포진해 우승경쟁에 합류했다 .\n",
      "ERROR\n",
      "▶ [NBA 파이널] 보스턴 우승 이끈 '완벽한 수비'\n",
      "ERROR\n",
      "올림픽 자유형 400m 안뛴다 ‘마린보이’ 박태환(단국대)의 유력한 베이징올림픽 금메달 종목인 자유형 400m에서 잠재적 경쟁자로 떠올랐던 ‘수영황제’ 마이클 펠프스(23 · 미국)가 이 종목 출전을 포기했다 .\n",
      "ERROR\n",
      "처음 립스틱을 접했을 때부터 붉은색만 쓰다 보니 새로운 색깔이 유행해도 선뜻 도전하기가 두려운 립스틱계 위정척사(?)파.▼네 개 브랜드 제품 평▼○디올 어딕트 립스틱 266 딜라이트(4만1000원/3.5g) 디올은 한 번 바르기만 해도 생기 넘치는 입술을 연출할 수 있는 어딕트 립스틱을 선보였다.\n",
      "ERROR\n",
      "49세. 고인은 중앙대 연극영화과를 졸업한 후 영화 ‘총잡이’ ‘본 투 킬’ 등의 조연출을 거쳐 2003년 ‘위대한 유산’으로 데뷔했다.\n",
      "ERROR\n",
      "문화 프로젝트 그룹 ‘결사대’는 31일 “제주시 삼도2동 간드락소극장에서 음악인형극 ‘아기사슴섬 천사들’을 공연한다”고 밝혔다.\n",
      "ERROR\n",
      "울산 모비스는 올해 NBA 서머리그에서 LA 레이커스 소속으로 뛴 브라이언트 던스턴(22 · 198.6㎝)을 지명했고 대구 오리온스는 가넷 톰슨(28 · 205㎝) .\n",
      "ERROR\n",
      "▶ '덩크왕' 창원 LG 석명준, 6월7일 웨딩마치\n",
      "ERROR\n",
      "정우성은 영화 ‘아수라’에서 말기 암 환자인 아내의 병원비를 핑계로 돈 되는 일은 뭐든지 하는 강력계 형사 한도경 역을 맡았다.\n",
      "ERROR\n",
      "[사진 아모레퍼시픽]\n",
      "ERROR\n",
      "[현대캐피탈 매튜 존 앤더슨 .\n",
      "ERROR\n",
      "●올림픽 국가대표 3차 평가전 최종(서울 태릉선수촌)\n",
      "ERROR\n",
      "당시 회사로부터 “경매를 진행해보지 않겠느냐”는 제안을 받은 그는 6개월이 넘게 연습을 한 뒤 2010년 6월 첫 경매에 나섰다.\n",
      "ERROR\n",
      "\"또 다시 성사된 통신사 매치!\", LCK 스프링 결승전 빅매치 성사\n",
      "ERROR\n",
      "‘제라드 퇴장 리버풀 맨유’ 소식을 접한 네티즌들은 “제라드 퇴장 리버풀 맨유, 제라드 아쉽다”,\n",
      "ERROR\n",
      "엑스맨 아포칼립스.엑스맨 아포칼립스, 1차 예고편 공개…신구 캐릭터 조합에 ‘기대감↑’영화 ‘엑스맨: 아포칼립스’ 1차 예고편이 15일 공개 됐다.\n",
      "ERROR\n",
      "\" 그러나 올시즌 최고의 식스맨 김일두(26 · 안양 KT&G)의 공백은 컸다 .\n",
      "ERROR\n",
      "영업이익으로 이자도 내지 못하는 ‘좀비’ 중소기업 105곳이 퇴출된다.\n",
      "ERROR\n",
      "[장내 · 장외] 이병규 2안타 2타점… 이승엽 무안타 外\n",
      "ERROR\n",
      "‘2015 무한도전 가요제’에는 가수 아이유, 밴드 혁오, 자이언티, 지드래곤, 태양, 박진영, 윤상 등이 참여한다.\n",
      "ERROR\n",
      "●아스널 1-1 리버풀 .\n",
      "ERROR\n",
      "원주 동부 프로미를 이끄는 전 감독은 7일 원주에서 열린 안양 KT&G 카이츠와의 2차전을 앞두고 전날 경기에 대해 \"KCC의 경기감각이 떨어진 반면, 삼성은 워낙 잘했다 .\n",
      "ERROR\n",
      "‘10년 전 모습 그대로’ 김남주-김승우 부부, 리마인드 웨딩 화보 공개…“우린 좋은 친구이자 동료”리마인드 웨딩 화보배우 김남주(44), 김승우(46) 부부가 결혼 10주년을 맞아 리마인드 웨딩 화보를 촬영했다.\n",
      "ERROR\n",
      "한편 ‘오늘 출소’ 고영욱은 앞으로 신상정보 공개 고지 5년, 위치추적 전자장치(전자발찌) 부착 3년이 추가 시행된다.\n",
      "ERROR\n",
      "타율은 전날 0．308에서 0．279로 하락했다 ．\n",
      "ERROR\n",
      "●세계유소년선수권대회(이탈리아 아씨레알레)\n",
      "ERROR\n",
      "NHN엔터, 모바일게임 ‘히어로즈킹덤’ 출시\n",
      "ERROR\n",
      "▲ 브라운의 3시즌 4월 한 달간 성적\n",
      "ERROR\n",
      "롯데호텔서울 본관 1층의 ‘더 라운지’와 신관 14층의 ‘살롱 드 떼’에서는 정오부터 오후 5시까지 애프터눈 티 세트를 판매한다.\n",
      "ERROR\n",
      "슈가볼, 새앨범 ‘유어스’ 발표\n",
      "ERROR\n",
      "세무당국은 부친이 A씨에게 주식을 증여한 것으로 보고 상속세 및 증여세법상 ‘명의신탁 증여의제’ 규정에 따라 2005~2008년분 증여세와 가산세 총 6억9460여만원을 부과했다.\n",
      "ERROR\n",
      "●수원 에두 4 연속 공격포인트(3골2도움) [인천유나이티드-성남일화](오후3시 · 인천 문학월드컵경기장)\n",
      "ERROR\n",
      "온라인 중앙일보 jstar@joongang.co.kr [사진 KBS 2TV 금요드라마 ‘오렌지 마말레이드’ 캡처]\n",
      "ERROR\n",
      "▼ 지금도 중국에 머물고 있나요?네. 〈나가수〉 무대 준비 때문에 계속 중국 후난성 성도 창사에서 지내고 있어요.\n",
      "ERROR\n",
      "‘이휘재 문정원’39금 토크쇼 ‘어쩌다 어른’ 새 MC 이휘재가 아내 문정원에 대해 언급했다.\n",
      "ERROR\n",
      "이때 선조는 기미상궁 김개시(김여진)가 준 탕약을 마시고 쓰러진 상태.놀란 광해군은 “어의를 불러라”고 외치며 선조의 옆을 지켰다.\n",
      "ERROR\n",
      "‘대한민국 알프스 하동’에서 제 20회 하동야생차 문화 축제가 펼쳐진다.\n",
      "ERROR\n",
      "쓰치야 요시히코(土屋義彦·2008년 작고·사진 왼쪽) 전 일본 참의원 의장의 장녀인 쓰치야 모모코(土屋桃子·65) 여사였다.\n",
      "ERROR\n",
      "‘여자친구’그룹 여자친구가 1위 공약을 내세웠다.\n",
      "ERROR\n",
      "이에 유재석은 ‘내 딸, 금사월’ 촬영에 합류해 1인 3역을 소화했다.\n",
      "ERROR\n",
      "10일 한국보건사회연구원이 발표한 ‘전국 출산력 및 가족보건·복지실태’에 따르면 15~49세 기혼여성의 평균 자녀 수는 1.75명이었다.\n",
      "ERROR\n",
      "‘가인 하와’가인의 네 번째 미니앨범 ‘하와’(Hawwah)의 더블 타이틀곡 ‘애플’(Apple)이 KBS 방송 부적격 판정을 받아 관심을 모았다.\n",
      "ERROR\n",
      "이를 듣던 MC 김제동은 \"크게 욕먹은 경험들이 변하게 만들었냐\"고 물었고, 정형돈은 \"그렇다.\n",
      "ERROR\n",
      "가세요’ 행복드림쉼터, 전국에 연내 40곳 설치\n",
      "ERROR\n",
      "10년 전 SK에 입단한 첫 해 그는 ‘1’할(0.193) 타자였다.\n",
      "ERROR\n",
      "‘SNL 코리아’ 예원 셀프 디스? “예의 바른 태도로 선배들에 예쁨 받아”가수 예원이 ‘셀프 디스’로 웃음을 자아냈다.\n",
      "스포츠동아 김민정 기자 ricky337@donga.com 스포츠동아[ORG] 김민정[PER] 기자 ricky337@donga.com[POH]\n",
      "이에 따라 MRO 사업 규모도 같은 기간 643억 달러에서 960억 달러까지 늘어나고 아시아·태평양 지역에서만도 2025년 336억 달러 규모의 시장이 형성될 것으로 예측되고 있다. 이에 따라 MRO 사업 규모도 같은 기간 643억 달러[MNY]에서 960억 달러[MNY]까지 늘어나고 아시아[LOC]·태평양[LOC] 지역에서만도 2025년[DAT] 336억 달러[MNY] 규모의 시장이 형성될 것으로 예측되고 있다.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import os\n",
    "answers = []\n",
    "with open('kobort/data/test.tsv', \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = csv.reader(f, delimiter='\\t',)\n",
    "    for i, line in enumerate(lines):\n",
    "        if i>0:\n",
    "            #print(line)\n",
    "            origin_text, _, tokenized_text, label_str = line\n",
    "            tokens = tokenized_text.split()\n",
    "            labels = label_str.split()\n",
    "            #print(origin_text)\n",
    "            #print(tokens)\n",
    "            #print(len(tokens))\n",
    "            #print(labels)\n",
    "            #print(len(labels))\n",
    "            tagged_sentence, tagged_id_list = return_ner_tagged_sentence_plus(origin_text, tokens,labels)\n",
    "            #print(tagged_sentence)\n",
    "            answers.append([origin_text,tagged_sentence])\n",
    "            #print(tagged_id_list)\n",
    "            #print()\n",
    "\n",
    "for i, answer in enumerate(answers):\n",
    "    if i<2:\n",
    "        print(answer[0], answer[1])\n",
    "\n",
    "with open(\"kobort/data/answer2.tsv\", \"w\", encoding=\"utf-8\") as f:\n",
    "    field_names = [\"origin_text\", \"ner_tagged_text\"]\n",
    "    writer = csv.writer(f, delimiter='\\t')\n",
    "    writer.writerow(field_names)\n",
    "    for answer in answers:\n",
    "        writer.writerow([answer[0], answer[1]])\n",
    "#            {'origin_text':answer[0], 'ner_tagged_text':answer[1]})\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "53778558-eef5-483e-a7a3-08cd25f263c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token : 스, label : B-ORG\n",
      "token : 포, label : I-ORG\n",
      "token : 츠, label : I-ORG\n",
      "token : 동, label : I-ORG\n",
      "token : 아, label : I-ORG\n",
      "token : 김, label : B-PER\n",
      "token : 민, label : I-PER\n",
      "token : 정, label : I-PER\n",
      "token : 기, label : O\n",
      "token : 자, label : O\n",
      "token : r, label : B-POH\n",
      "token : i, label : I-POH\n",
      "token : c, label : I-POH\n",
      "token : k, label : I-POH\n",
      "token : y, label : I-POH\n",
      "token : 3, label : I-POH\n",
      "token : 3, label : I-POH\n",
      "token : 7, label : I-POH\n",
      "token : @, label : I-POH\n",
      "token : d, label : I-POH\n",
      "token : o, label : I-POH\n",
      "token : n, label : I-POH\n",
      "token : g, label : I-POH\n",
      "token : a, label : I-POH\n",
      "token : ., label : I-POH\n",
      "token : c, label : I-POH\n",
      "token : o, label : I-POH\n",
      "token : m, label : I-POH\n",
      "스포츠동아[ORG] 김민정[PER] 기자 ricky337@donga.com[POH]\n",
      "스포츠동아\n",
      "김민정\n",
      "ricky337@donga.com\n"
     ]
    }
   ],
   "source": [
    "origin_text = '스포츠동아 김민정 기자 ricky337@donga.com'\n",
    "tokens = ['스포츠', '##동', '##아', '김민', '##정', '기자', 'r', '##ick', '##y', '33', '##7', '@', 'do', '##ng', '##a', '.', 'com']\n",
    "labels = ['B-ORG', 'I-ORG', 'I-ORG', 'B-PER', 'I-PER', 'O', 'B-POH', 'I-POH', 'I-POH', 'I-POH', 'I-POH', 'I-POH', 'I-POH', 'I-POH', 'I-POH', 'I-POH', 'I-POH']\n",
    "ner_tagged_sentence, ner_tagged_id_list = return_ner_tagged_sentence(origin_text, tokens,labels)\n",
    "print(ner_tagged_sentence)\n",
    "for item in ner_tagged_id_list:\n",
    "    start, end = item\n",
    "    print(origin_text[start:end+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a38d3d-093b-4f18-8c0a-aa0c7fa8d4ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "43f4d214-4e0c-495c-9595-6ee8da227df5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token : 오, label : O\n",
      "token : 늘, label : O\n",
      "token : 고, label : B-PER\n",
      "token : 보, label : I-PER\n",
      "token : 원, label : I-PER\n",
      "token : 은, label : O\n",
      "token : 9, label : B-TIM\n",
      "token : 시, label : I-TIM\n",
      "token : 에, label : O\n",
      "token : 티, label : B-ORG\n",
      "token : 맥, label : I-ORG\n",
      "token : 스, label : I-ORG\n",
      "token : 로, label : O\n",
      "token : 출, label : O\n",
      "token : 근, label : O\n",
      "token : 했, label : O\n",
      "token : 다, label : O\n",
      "오늘 고보원[PER]은 9시[TIM]에 티맥스[ORG]로 출근했다\n",
      "고보원\n",
      "9시\n",
      "티맥스\n"
     ]
    }
   ],
   "source": [
    "origin_text = '오늘 고보원은 9시에 티맥스로 출근했다'\n",
    "tokens = ['오늘', '고', '##보', '##원', '은', '9', '시', '에', '티', '##맥스', '로', '출근', '했', '다']\n",
    "labels = ['O', 'B-PER', 'I-PER', 'I-PER', 'O', 'B-TIM', 'I-TIM', 'O', 'B-ORG', 'I-ORG', 'O', 'O', 'O', 'O']\n",
    "ner_tagged_sentence, ner_tagged_id_list = return_ner_tagged_sentence(origin_text, tokens,labels)\n",
    "print(ner_tagged_sentence)\n",
    "for item in ner_tagged_id_list:\n",
    "    start, end = item\n",
    "    print(origin_text[start:end+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "af8d5bc5-d055-4e4d-ab2f-89ea82221658",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token : 세, label : O\n",
      "token : 련, label : O\n",
      "token : 미, label : O\n",
      "token : 가, label : O\n",
      "token : 미, label : O\n",
      "token : 한, label : O\n",
      "token : 공, label : O\n",
      "token : 격, label : O\n",
      "token : 우, label : B-ORG\n",
      "token : 리, label : I-ORG\n",
      "token : 은, label : I-ORG\n",
      "token : 행, label : I-ORG\n",
      "token : 박, label : B-PER\n",
      "token : 혜, label : I-PER\n",
      "token : 진, label : I-PER\n",
      "token : ,, label : O\n",
      "token : 임, label : B-PER\n",
      "token : 영, label : I-PER\n",
      "token : 희, label : I-PER\n",
      "token : ,, label : O\n",
      "token : 양, label : B-PER\n",
      "token : 지, label : I-PER\n",
      "token : 희, label : I-PER\n",
      "token : 는, label : O\n",
      "token : 공, label : O\n",
      "token : 격, label : O\n",
      "token : 에, label : O\n",
      "token : 도, label : O\n",
      "token : 능, label : O\n",
      "token : 하, label : O\n",
      "token : 다, label : O\n",
      "token : ., label : O\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('세련미 가미한 공격 우리은행[ORG] 박혜진[PER], 임영희[PER], 양지희[PER]는 공격에도 능하다.',\n",
       " [[11, 14], [16, 18], [21, 23], [26, 28]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "origin_text = \"세련미 가미한 공격 우리은행 박혜진, 임영희, 양지희는 공격에도 능하다.\"\n",
    "tokens = ['세련', '##미', '가', '미', '한', '공격', '우리', '##은행', '박', '##혜진', ',', '임영', '##희', ',', '양지', '##희', '는', '공격', '에', '도', '능하', '다', '.']\n",
    "labels = ['O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'I-ORG', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
    "return_ner_tagged_sentence(origin_text, tokens,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "b03182e5-afe1-4960-8d87-3529d64ade9c",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tweepy' has no attribute 'StreamListener'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_85020/3493932124.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmake_tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0manswers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"test.tsv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/bowon_ko/kobort/tokenizer/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmake_tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/bowon_ko/kobort/tokenizer/tokenize.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBertTokenizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mAlbertTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkonlpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMecab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mwordpiece_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"vocab_path\"\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;34m\"tokenizer/model/wordpiece/version_1.4\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pretrain/lib/python3.7/site-packages/konlpy/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkonlpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjvm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minit_jvm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m from konlpy import (\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pretrain/lib/python3.7/site-packages/konlpy/stream/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkonlpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBaseStreamer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mKonlpyStreamerError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkonlpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtwitter\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTwitterStreamer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkonlpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnaver\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNaverStreamer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkonlpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdcinside\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDCInsideStreamer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pretrain/lib/python3.7/site-packages/konlpy/stream/twitter.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mCorpusListener\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweepy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStreamListener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \"\"\"CorpusListener is a tweepy listener to listen on filtered list of words.\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tweepy' has no attribute 'StreamListener'"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from tokenizer import make_tokens\n",
    "answers = []\n",
    "with open(\"test.tsv\", \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = csv.reader(f, delimiter='\\t',)\n",
    "    for i, line in enumerate(lines):\n",
    "        if i>0:\n",
    "            #print(line)\n",
    "            origin_text, _, _, label_str = line\n",
    "            morphed_text = make_tokens(origin_text, model_name=\"mecab\")\n",
    "            tokenized_text = make_tokens(origin_text, model_name=\"wp-mecab\")\n",
    "            tokenized_text = ' '.join(tokenized_text)\n",
    "            answers.append([origin_text, morphed_text, tokenized_text, label_str])\n",
    "\n",
    "for i, answer in enumerate(answers):\n",
    "    if i<2:\n",
    "        print(answer[0], answer[1], answer[2], answer[3])\n",
    "\n",
    "with open(\"test_v1.1.tsv\", \"w\", encoding=\"utf-8\") as f:\n",
    "    field_names = [\"original_text\", \"morphed_text\", \"tokenize\", \"bio_tagging\"]\n",
    "    writer = csv.writer(f, delimiter='\\t')\n",
    "    writer.writerow(field_names)\n",
    "    for answer in answers:\n",
    "        writer.writerow([answer[0], answer[1], answer[2], answer[3]])\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "0c19ecc1-ec96-4c0b-816c-18d5ed077ddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tweepy==3.10.0\n",
      "  Using cached tweepy-3.10.0-py2.py3-none-any.whl (30 kB)\n",
      "Requirement already satisfied: requests[socks]>=2.11.1 in /home/ubuntu/anaconda3/envs/pretrain/lib/python3.7/site-packages (from tweepy==3.10.0) (2.26.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/ubuntu/anaconda3/envs/pretrain/lib/python3.7/site-packages (from tweepy==3.10.0) (1.3.0)\n",
      "Requirement already satisfied: six>=1.10.0 in /home/ubuntu/anaconda3/envs/pretrain/lib/python3.7/site-packages (from tweepy==3.10.0) (1.16.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/ubuntu/anaconda3/envs/pretrain/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->tweepy==3.10.0) (3.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ubuntu/anaconda3/envs/pretrain/lib/python3.7/site-packages (from requests[socks]>=2.11.1->tweepy==3.10.0) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ubuntu/anaconda3/envs/pretrain/lib/python3.7/site-packages (from requests[socks]>=2.11.1->tweepy==3.10.0) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ubuntu/anaconda3/envs/pretrain/lib/python3.7/site-packages (from requests[socks]>=2.11.1->tweepy==3.10.0) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/ubuntu/anaconda3/envs/pretrain/lib/python3.7/site-packages (from requests[socks]>=2.11.1->tweepy==3.10.0) (2.0.7)\n",
      "Collecting PySocks!=1.5.7,>=1.5.6\n",
      "  Using cached PySocks-1.7.1-py3-none-any.whl (16 kB)\n",
      "Installing collected packages: PySocks, tweepy\n",
      "  Attempting uninstall: tweepy\n",
      "    Found existing installation: tweepy 4.4.0\n",
      "    Uninstalling tweepy-4.4.0:\n",
      "      Successfully uninstalled tweepy-4.4.0\n",
      "Successfully installed PySocks-1.7.1 tweepy-3.10.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tweepy==3.10.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5ca9b5-b19b-4397-b380-43cff85a150a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
