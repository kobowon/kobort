{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb342377-1a90-4e6d-9ce4-9058d5a2b795",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer,AlbertTokenizer\n",
    "from konlpy.tag import Mecab\n",
    "\n",
    "wordpiece_info = {\"vocab_path\" : \"tokenizer/model/wordpiece/version_1.4\"}\n",
    "\n",
    "wordpiece_mecab_info = {\"vocab_path\" : \"/data/bowon_ko/wordpiece/version_1.9\"}\n",
    "\n",
    "sentencepiece_mecab_info = {\"vocab_path\" : \"tokenizer/model/sentencepiece_mecab/version_0.1/version_0.1.model\"}\n",
    "\n",
    "# def load_tokenizer(model_name=\"wp-mecab\"):\n",
    "    \n",
    "\n",
    "\n",
    "def make_tokens(text, model_name=\"wp-mecab\"):\n",
    "    if model_name == \"sp-mecab\":\n",
    "        tokenizer_path = sentencepiece_mecab_info[\"vocab_path\"]\n",
    "        tokenizer = AlbertTokenizer.from_pretrained(tokenizer_path,\n",
    "                                                do_lower_case=False,\n",
    "                                                unk_token='<unk>',\n",
    "                                                sep_token='</s>',\n",
    "                                                pad_token='<pad>',\n",
    "                                                cls_token='<s>',\n",
    "                                                mask_token='<mask>',\n",
    "                                                use_fast=True)\n",
    "    elif \"wp\" in model_name:\n",
    "        if model_name == \"wp-mecab\":\n",
    "            tokenizer_path = wordpiece_mecab_info[\"vocab_path\"]\n",
    "        elif model_name == \"wp\":\n",
    "            tokenizer_path = wordpiece_info[\"vocab_path\"]\n",
    "        tokenizer = BertTokenizer.from_pretrained(tokenizer_path, \n",
    "                                              do_lower_case=False,\n",
    "                                              unk_token='<unk>',\n",
    "                                              sep_token='</s>',\n",
    "                                              pad_token='<pad>',\n",
    "                                              cls_token='<s>',\n",
    "                                              mask_token='<mask>',\n",
    "                                              use_fast=True)\n",
    "    \n",
    "    if \"mecab\" in model_name or model_name==\"mecab\":\n",
    "        mecab = Mecab()\n",
    "        #text를 형태소로 분절 및 결합\n",
    "        morphs = mecab.morphs(text)\n",
    "        text = \" \".join(morphs)\n",
    "        if model_name == \"mecab\":\n",
    "            return text\n",
    "        \n",
    "    #텍스트를 토크나이즈\n",
    "    tokens = tokenizer.encode(text)\n",
    "    \n",
    "    return tokens\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6f3cc3a-231c-4bd4-b342-f71f461c84b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "file /data/bowon_ko/wordpiece/version_1.9/config.json not found\n"
     ]
    }
   ],
   "source": [
    "tokenizer_path = wordpiece_mecab_info[\"vocab_path\"]\n",
    "tokenizer = BertTokenizer.from_pretrained(\"/data/bowon_ko/wordpiece/version_1.9\", \n",
    "                                              do_lower_case=False,\n",
    "                                              unk_token='<unk>',\n",
    "                                              sep_token='</s>',\n",
    "                                              pad_token='<pad>',\n",
    "                                              cls_token='<s>',\n",
    "                                              mask_token='<mask>',\n",
    "                                              use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f83b9be0-de45-4abe-8637-c89719a84edb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "file /data/bowon_ko/wordpiece/version_1.9/config.json not found\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"/data/bowon_ko/wordpiece/version_1.9\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ed465d3-cf3d-46ff-bf88-578fc3803db4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('unk_token', '[UNK]'), ('sep_token', '[SEP]'), ('pad_token', '[PAD]'), ('cls_token', '[CLS]'), ('mask_token', '[MASK]')])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.special_tokens_map.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4bd0da1f-f53c-45bc-8cdc-e6e67c61bc56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 3, 274, 16772, 24512, 6549, 18085, 38598, 6501, 22120, 6242, 6318, 15182, 11557, 2774, 6724, 6562, 2]\n"
     ]
    }
   ],
   "source": [
    "output = tokenizer.encode(\"ㆍ정의화 number일 직권상정여 쟁점법안 연계 처리 불투명\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16a2cc2a-96fd-4d11-a5ef-3e837388f1bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s> <unk> number일 직권상정여 쟁점법안 연계 처리 불투명 </s>'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dcc4cd85-08c6-4f48-b335-9ac519f051f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('unk_token', '<unk>'), ('sep_token', '</s>'), ('pad_token', '<pad>'), ('cls_token', '<s>'), ('mask_token', '<mask>')])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.special_tokens_map.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9aad242a-7a3b-4e08-a162-a522fb336d43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<unk>', '</s>', '<pad>', '<s>', '<mask>']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.all_special_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1bd2270c-2fd5-47d3-b5ab-a4e6f60b0023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unk_token 0\n",
      "sep_token 1\n",
      "pad_token 2\n",
      "cls_token 3\n",
      "mask_token 4\n"
     ]
    }
   ],
   "source": [
    "token_id = {}\n",
    "for k, v in tokenizer.special_tokens_map.items():\n",
    "    idx = tokenizer.all_special_tokens.index(v)\n",
    "    print(k,idx)\n",
    "    token_id[k] = idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4530c3c5-2158-4fd4-a684-928402821cc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('unk_token', 0), ('sep_token', 1), ('pad_token', 2), ('cls_token', 3), ('mask_token', 4)])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_id.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6983d8ca-c7c3-4a13-b38e-e51bfd511068",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values([0, 1, 2, 3, 4])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_id.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "95f17613-58e8-49f0-ae3c-97cb414cd92d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bert-base-uncased': 512,\n",
       " 'bert-large-uncased': 512,\n",
       " 'bert-base-cased': 512,\n",
       " 'bert-large-cased': 512,\n",
       " 'bert-base-multilingual-uncased': 512,\n",
       " 'bert-base-multilingual-cased': 512,\n",
       " 'bert-base-chinese': 512,\n",
       " 'bert-base-german-cased': 512,\n",
       " 'bert-large-uncased-whole-word-masking': 512,\n",
       " 'bert-large-cased-whole-word-masking': 512,\n",
       " 'bert-large-uncased-whole-word-masking-finetuned-squad': 512,\n",
       " 'bert-large-cased-whole-word-masking-finetuned-squad': 512,\n",
       " 'bert-base-cased-finetuned-mrpc': 512,\n",
       " 'bert-base-german-dbmdz-cased': 512,\n",
       " 'bert-base-german-dbmdz-uncased': 512,\n",
       " 'TurkuNLP/bert-base-finnish-cased-v1': 512,\n",
       " 'TurkuNLP/bert-base-finnish-uncased-v1': 512,\n",
       " 'wietsedv/bert-base-dutch-cased': 512}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.max_model_input_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b495f22-905c-4150-a66a-7457d6727293",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
